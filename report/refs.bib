
@article{baevskiData2vecGeneralFramework2022,
  title = {Data2vec: {{A General Framework}} for {{Self-supervised Learning}} in {{Speech}}, {{Vision}} and {{Language}}},
  shorttitle = {Data2vec},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  year = {2022},
  month = apr,
  journal = {arXiv:2202.03555 [cs]},
  eprint = {2202.03555},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/siddhantray/Zotero/storage/EJVRNSTI/Baevski et al. - 2022 - data2vec A General Framework for Self-supervised .pdf;/Users/siddhantray/Zotero/storage/JTRHEY9V/2202.html}
}

@article{beltagyLongformerLongDocumentTransformer2020,
  title = {Longformer: {{The Long-Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  year = {2020},
  month = dec,
  journal = {arXiv:2004.05150 [cs]},
  eprint = {2004.05150},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/siddhantray/Zotero/storage/UJPK2ACQ/Beltagy et al. - 2020 - Longformer The Long-Document Transformer.pdf;/Users/siddhantray/Zotero/storage/GJKBG4VZ/2004.html}
}

@article{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  journal = {arXiv:2005.14165 [cs]},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/siddhantray/Zotero/storage/DKX47QSK/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/siddhantray/Zotero/storage/2Z8JUV56/2005.html}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/siddhantray/Zotero/storage/2YT9M2VZ/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/siddhantray/Zotero/storage/6H2S6WY2/1810.html}
}

@article{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  journal = {arXiv:2010.11929 [cs]},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/siddhantray/Zotero/storage/6D65SQTJ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/siddhantray/Zotero/storage/832627XR/2010.html}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  publisher = {{MIT Press}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.``Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.''\textemdash Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  googlebooks = {Np9SDQAAQBAJ},
  isbn = {978-0-262-03561-3},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science}
}

@misc{heMaskedAutoencodersAre2021,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2021},
  month = dec,
  number = {arXiv:2111.06377},
  eprint = {2111.06377},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/siddhantray/Zotero/storage/LVGPABTP/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf;/Users/siddhantray/Zotero/storage/IQXS7TNN/2111.html}
}

@inproceedings{hePERTPayloadEncoding2020,
  title = {{{PERT}}: {{Payload Encoding Representation}} from {{Transformer}} for {{Encrypted Traffic Classification}}},
  shorttitle = {{{PERT}}},
  booktitle = {2020 {{ITU Kaleidoscope}}: {{Industry-Driven Digital Transformation}} ({{ITU K}})},
  author = {He, Hong Ye and Guo Yang, Zhi and Chen, Xiang Ning},
  year = {2020},
  month = dec,
  pages = {1--8},
  doi = {10.23919/ITUK50268.2020.9303204},
  abstract = {Traffic identification becomes more important yet more challenging as related encryption techniques are rapidly developing nowadays. In difference to recent deep learning methods that apply image processing to solve such encrypted traffic problems, in this paper, we propose a method named Payload Encoding Representation from Transformer (PERT) to perform automatic traffic feature extraction using a state-of-the-art dynamic word embedding technique. Based on this, we further provide a traffic classification framework in which unlabeled traffic is utilized to pre-train an encoding network that learns the contextual distribution of traffic payload bytes. Then, the downward classification reuses the pre-trained network to obtain an enhanced classification result. By implementing experiments on a public encrypted traffic data set and our captured Android HTTPS traffic, we prove the proposed method can achieve an obvious better effectiveness than other compared baselines. To the best of our knowledge, this is the first time the encrypted traffic classification with the dynamic word embedding alone with its pre-training strategy has been addressed.},
  keywords = {Cryptography,Deep learning,dynamic word embedding,encrypted traffic classification,Feature extraction,Image coding,natural language processing,Payloads,Task analysis,Telecommunication traffic,traffic identification},
  file = {/Users/siddhantray/Zotero/storage/NJPKY52R/He et al. - 2020 - PERT Payload Encoding Representation from Transfo.pdf;/Users/siddhantray/Zotero/storage/7RXZ67EC/9303204.html}
}

@inproceedings{jayDeepReinforcementLearning2019,
  title = {A {{Deep Reinforcement Learning Perspective}} on {{Internet Congestion Control}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Jay, Nathan and Rotman, Noga and Godfrey, Brighten and Schapira, Michael and Tamar, Aviv},
  year = {2019},
  month = may,
  pages = {3050--3059},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources' data-transmission rates to efficiently utilize network capacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality, Internet-of-Things, and more. We show that casting congestion control as RL enables training deep network policies that capture intricate patterns in data traffic and network conditions, and leverage this to outperform the state-of-the-art. We also highlight significant challenges facing real-world adoption of RL-based congestion control, including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reproducibility of our results, we present a test suite for RL-guided congestion control based on the OpenAI Gym interface.},
  langid = {english},
  file = {/Users/siddhantray/Zotero/storage/CPD88YJG/Jay et al. - 2019 - A Deep Reinforcement Learning Perspective on Inter.pdf}
}

@inproceedings{maoNeuralAdaptiveVideo2017,
  title = {Neural {{Adaptive Video Streaming}} with {{Pensieve}}},
  booktitle = {Proceedings of the {{Conference}} of the {{ACM Special Interest Group}} on {{Data Communication}}},
  author = {Mao, Hongzi and Netravali, Ravi and Alizadeh, Mohammad},
  year = {2017},
  month = aug,
  pages = {197--210},
  publisher = {{ACM}},
  address = {{Los Angeles CA USA}},
  doi = {10.1145/3098822.3098843},
  abstract = {Client-side video players employ adaptive bitrate (ABR) algorithms to optimize user quality of experience (QoE). Despite the abundance of recently proposed schemes, state-of-the-art ABR algorithms suffer from a key limitation: they use fixed control rules based on simplified or inaccurate models of the deployment environment. As a result, existing schemes inevitably fail to achieve optimal performance across a broad set of network conditions and QoE objectives. We propose Pensieve, a system that generates ABR algorithms using reinforcement learning (RL). Pensieve trains a neural network model that selects bitrates for future video chunks based on observations collected by client video players. Pensieve does not rely on pre-programmed models or assumptions about the environment. Instead, it learns to make ABR decisions solely through observations of the resulting performance of past decisions. As a result, Pensieve automatically learns ABR algorithms that adapt to a wide range of environments and QoE metrics. We compare Pensieve to state-of-theart ABR algorithms using trace-driven and real world experiments spanning a wide variety of network conditions, QoE metrics, and video properties. In all considered scenarios, Pensieve outperforms the best state-of-the-art scheme, with improvements in average QoE of 12\%\textendash 25\%. Pensieve also generalizes well, outperforming existing schemes even on networks for which it was not explicitly trained.},
  isbn = {978-1-4503-4653-5},
  langid = {english},
  file = {/Users/siddhantray/Zotero/storage/XCG5ZBUF/Mao et al. - 2017 - Neural Adaptive Video Streaming with Pensieve.pdf}
}

@misc{nickelPoincarEmbeddingsLearning2017,
  title = {Poincar\textbackslash 'e {{Embeddings}} for {{Learning Hierarchical Representations}}},
  author = {Nickel, Maximilian and Kiela, Douwe},
  year = {2017},
  month = may,
  number = {arXiv:1705.08039},
  eprint = {1705.08039},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\textbackslash 'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\textbackslash 'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/siddhantray/Zotero/storage/N33SFVNB/Nickel and Kiela - 2017 - Poincar'e Embeddings for Learning Hierarchical Re.pdf;/Users/siddhantray/Zotero/storage/ZZ2XINT2/1705.html}
}

@misc{sarkarNs3dumbelltopologysimulation2022,
  title = {Ns3-Dumbell-Topology-Simulation},
  author = {Sarkar, Pritam},
  year = {2022},
  month = mar,
  abstract = {Analyze and compare TCP Reno, TCP Westwood, and TCP Fack performance using NS3 simulator},
  copyright = {MIT},
  keywords = {congestion-loss,dumbbell-topology,ns3-simulator,routers,tcp-reno,tcp-westwood,throughput,topology}
}

@misc{Spearmint2020,
  title = {Spearmint},
  year = {2020},
  month = mar,
  abstract = {Spearmint Bayesian optimization codebase},
  howpublished = {Stanford Systems and Networking Research}
}


@inproceedings{vaswaniAttentionAllYou2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@misc{wettigShouldYouMask,
  doi = {10.48550/ARXIV.2202.08005},
  
  url = {https://arxiv.org/abs/2202.08005},
  
  author = {Wettig, Alexander and Gao, Tianyu and Zhong, Zexuan and Chen, Danqi},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Should You Mask 15\% in Masked Language Modeling?},
  
  publisher = {arXiv},
  month = {May},
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@misc{xiaAutomaticCurriculumGeneration2022,
  title = {Automatic {{Curriculum Generation}} for {{Learning Adaptation}} in {{Networking}}},
  author = {Xia, Zhengxu and Zhou, Yajie and Yan, Francis Y. and Jiang, Junchen},
  year = {2022},
  month = feb,
  number = {arXiv:2202.05940},
  eprint = {2202.05940},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {As deep reinforcement learning (RL) showcases its strengths in networking and systems, its pitfalls also come to the public's attention--when trained to handle a wide range of network workloads and previously unseen deployment environments, RL policies often manifest suboptimal performance and poor generalizability. To tackle these problems, we present Genet, a new training framework for learning better RL-based network adaptation algorithms. Genet is built on the concept of curriculum learning, which has proved effective against similar issues in other domains where RL is extensively employed. At a high level, curriculum learning gradually presents more difficult environments to the training, rather than choosing them randomly, so that the current RL model can make meaningful progress in training. However, applying curriculum learning in networking is challenging because it remains unknown how to measure the "difficulty" of a network environment. Instead of relying on handcrafted heuristics to determine the environment's difficulty level, our insight is to utilize traditional rule-based (non-RL) baselines: If the current RL model performs significantly worse in a network environment than the baselines, then the model's potential to improve when further trained in this environment is substantial. Therefore, Genet automatically searches for the environments where the current model falls significantly behind a traditional baseline scheme and iteratively promotes these environments as the training progresses. Through evaluating Genet on three use cases--adaptive video streaming, congestion control, and load balancing, we show that Genet produces RL policies which outperform both regularly trained RL policies and traditional baselines in each context, not only under synthetic workloads but also in real environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Networking and Internet Architecture},
  file = {/Users/siddhantray/Zotero/storage/XRSJ8ZTI/Xia et al. - 2022 - Automatic Curriculum Generation for Learning Adapt.pdf;/Users/siddhantray/Zotero/storage/MEQ4UVFL/2202.html}
}

@article{yanLearningSituRandomized,
  title = {Learning in Situ: A Randomized Experiment in Video Streaming},
  author = {Yan, Francis Y and Hong, James and Ayers, Hudson and Zhang, Keyi and Zhu, Chenzhi and Levis, Philip and Fouladi, Sadjad and Winstein, Keith},
  pages = {18},
  abstract = {We describe the results of a randomized controlled trial of video-streaming algorithms for bitrate selection and network prediction. Over the last year, we have streamed 38.6 years of video to 63,508 users across the Internet. Sessions are randomized in blinded fashion among algorithms.},
  langid = {english},
  file = {/Users/siddhantray/Zotero/storage/JFIJ86KI/Yan et al. - Learning in situ a randomized experiment in video.pdf}
}

@article{yanPantheonTrainingGround,
  title = {Pantheon: The Training Ground for {{Internet}} Congestion-Control Research},
  author = {Yan, Francis Y and Ma, Jestin and Hill, Greg D and Raghavan, Deepti and Wahby, Riad S and Levis, Philip and Winstein, Keith},
  pages = {13},
  abstract = {Internet transport algorithms are foundational to the performance of network applications. But a number of practical challenges make it difficult to evaluate new ideas and algorithms in a reproducible manner. We present the Pantheon, a system that addresses this by serving as a community ``training ground'' for research on Internet transport protocols and congestion control (https: //pantheon.stanford.edu). It allows network researchers to benefit from and contribute to a common set of benchmark algorithms, a shared evaluation platform, and a public archive of results.},
  langid = {english},
  file = {/Users/siddhantray/Zotero/storage/2YYZCDPL/Yan et al. - Pantheon the training ground for Internet congest.pdf}
}

@inproceedings{zaheerDeepSets2018,
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbhakhsh, Siamak and P\'{o}czos, Barnab\'{a}s and Salakhutdinov, Ruslan and Smola, Alexander J},
title = {Deep Sets},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics [1], to anomaly detection in piezometer data of embankment dams [2], to cosmology [3, 4]. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {3394–3404},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{zhangMimicNetFastPerformance2021,
  title = {{{MimicNet}}: Fast Performance Estimates for Data Center Networks with Machine Learning},
  shorttitle = {{{MimicNet}}},
  booktitle = {Proceedings of the 2021 {{ACM SIGCOMM}} 2021 {{Conference}}},
  author = {Zhang, Qizhen and Ng, Kelvin K. W. and Kazer, Charles and Yan, Shen and Sedoc, Jo{\~a}o and Liu, Vincent},
  year = {2021},
  month = aug,
  pages = {287--304},
  publisher = {{ACM}},
  address = {{Virtual Event USA}},
  doi = {10.1145/3452296.3472926},
  abstract = {At-scale evaluation of new data center network innovations is becoming increasingly intractable. This is true for testbeds, where few, if any, can afford a dedicated, full-scale replica of a data center. It is also true for simulations, which while originally designed for precisely this purpose, have struggled to cope with the size of today's networks.},
  isbn = {978-1-4503-8383-7},
  langid = {english},
  file = {/Users/siddhantray/Zotero/storage/EGUVK8RD/Zhang et al. - 2021 - MimicNet fast performance estimates for data cent.pdf}
}

@inproceedings{zhuNetworkPlanningDeep2021,
  title = {Network Planning with Deep Reinforcement Learning},
  booktitle = {Proceedings of the 2021 {{ACM SIGCOMM}} 2021 {{Conference}}},
  author = {Zhu, Hang and Gupta, Varun and Ahuja, Satyajeet Singh and Tian, Yuandong and Zhang, Ying and Jin, Xin},
  year = {2021},
  month = aug,
  pages = {258--271},
  publisher = {{ACM}},
  address = {{Virtual Event USA}},
  doi = {10.1145/3452296.3472902},
  abstract = {Network planning is critical to the performance, reliability and cost of web services. This problem is typically formulated as an Integer Linear Programming (ILP) problem. Today's practice relies on handtuned heuristics from human experts to address the scalability challenge of ILP solvers. In this paper, we propose NeuroPlan, a deep reinforcement learning (RL) approach to solve the network planning problem. This problem involves multi-step decision making and cost minimization, which can be naturally cast as a deep RL problem. We develop two important domain-specific techniques. First, we use a graph neural network (GNN) and a novel domain-specific node-link transformation for state encoding, in order to handle the dynamic nature of the evolving network topology during planning decision making. Second, we leverage a two-stage hybrid approach that first uses deep RL to prune the search space and then uses an ILP solver to find the optimal solution. This approach resembles today's practice, but avoids human experts with an RL agent in the first stage. Evaluation on real topologies and setups from large production networks demonstrates that NeuroPlan scales to large topologies beyond the capability of ILP solvers, and reduces the cost by up to 17\% compared to hand-tuned heuristics.},
  isbn = {978-1-4503-8383-7},
  langid = {english},
  file = {/Users/siddhantray/Zotero/storage/96T28PR9/Zhu et al. - 2021 - Network planning with deep reinforcement learning.pdf}
}

@article{Robbins2007ASA,
  title={A Stochastic Approximation Method},
  author={Herbert E. Robbins},
  journal={Annals of Mathematical Statistics},
  year={2007},
  volume={22},
  pages={400-407}
}

@inproceedings{rnnattention,
title = "Neural machine translation by jointly learning to align and translate",
author = "Dzmitry Bahdanau and Cho, {Kyung Hyun} and Yoshua Bengio",
year = "2015",
month = jan,
day = "1",
language = "English (US)",
booktitle = "3rd International Conference on Learning Representations, ICLR 2015 ; Conference date: 07-05-2015 Through 09-05-2015",
}

@misc{trans,
  title = {The Illustrated Transformer},
  author = {Jay Alammar},
  howpublished = {https://jalammar.github.io/illustrated-transformer/},
  note = {Accessed: 2022-07-15}
}

@misc{shaw2018selfattention,
      title={Self-Attention with Relative Position Representations}, 
      author={Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
      year={2018},
      eprint={1803.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gehring2017convolutional,
author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
title = {Convolutional Sequence to Sequence Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.*},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1243–1252},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@Inbook{ns3,
author="Riley, George F.
and Henderson, Thomas R.",
title="The ns-3 Network Simulator",
bookTitle="Modeling and Tools for Network Simulation",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="15--34",
abstract="As networks of computing devices grow larger and more complex, the need for highly accurate and scalable network simulation technologies becomes critical. Despite the emergence of large-scale testbeds for network research, simulation still plays a vital role in terms of scalability (both in size and in experimental speed), reproducibility, rapid prototyping, and education. With simulation based studies, the approach can be studied in detail at varying scales, with varying data applications, varying field conditions, and will result in reproducible and analyzable results.",
isbn="978-3-642-12331-3",
doi="10.1007/978-3-642-12331-3_2",
url="https://doi.org/10.1007/978-3-642-12331-3_2"
}

@article{generalizingdnn,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{homa,
author = {Montazeri, Behnam and Li, Yilong and Alizadeh, Mohammad and Ousterhout, John},
title = {Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230564},
doi = {10.1145/3230543.3230564},
abstract = {Homa is a new transport protocol for datacenter networks. It provides exceptionally low latency, especially for workloads with a high volume of very short messages, and it also supports large messages and high network utilization. Homa uses in-network priority queues to ensure low latency for short messages; priority allocation is managed dynamically by each receiver and integrated with a receiver-driven flow control mechanism. Homa also uses controlled overcommitment of receiver downlinks to ensure efficient bandwidth utilization at high load. Our implementation of Homa delivers 99th percentile round-trip times less than 15 μs for short messages on a 10 Gbps network running at 80% load. These latencies are almost 100x lower than the best published measurements of an implementation. In simulations, Homa's latency is roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {221–235},
numpages = {15},
keywords = {data centers, network stacks, low latency, transport protocols},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@article{closemask,
author = {Wilson L. Taylor},
title ={Cloze Procedure: A New Tool for Measuring Readability},
journal = {Journalism Quarterly},
volume = {30},
number = {4},
pages = {415-433},
year = {1953},
doi = {10.1177/107769905303000401},

URL = {https://doi.org/10.1177/107769905303000401
    
},
eprint = {https://doi.org/10.1177/107769905303000401
    
},
abstract = { Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which “cloze procedure” results are compared with those of two readability formulas. }
}

@inproceedings{poolcv,
  author={Yandex, Artem Babenko and Lempitsky, Victor},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Aggregating Local Deep Features for Image Retrieval}, 
  year={2015},
  volume={},
  number={},
  pages={1269-1277},
  doi={10.1109/ICCV.2015.150}}

@book{scaling,
author = {Grus, Joel},
title = {Data Science from Scratch: First Principles with Python},
year = {2015},
isbn = {149190142X},
publisher = {O'Reilly Media, Inc.},
edition = {1st},
abstract = {Data science libraries, frameworks, modules, and toolkits are great for doing data science, but they're also a good way to dive into the discipline without actually understanding data science. In this book, you'll learn how many of the most fundamental data science tools and algorithms work by implementing them from scratch. If you have an aptitude for mathematics and some programming skills, author Joel Grus will help you get comfortable with the math and statistics at the core of data science, and with hacking skills you need to get started as a data scientist. Today's messy glut of data holds answers to questions no one's even thought to ask. This book provides you with the know-how to dig those answers out.}
}




